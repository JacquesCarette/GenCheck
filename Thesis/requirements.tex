The sampling based \pbt software discussed in chapter \ref{pbtsystems}
have a number of similarities in their design
while adopting a variety of test selection and generation strategies.
The number of different packages suggests 
a strong interest in this approach to testing Haskell modules,
but the proliferation of independent software testing systems 
hinders comparisons of the different methodologies and their usefulness in software verification.

One of our main criticisms of the ``*\_Check'' family of testing systems
was their tight coupling of a test case generation, evaluation and reporting
around a single sampling method (section \ref{pbtsystems}).
Instead of developing independent testing systems for each sampling method,
a test system should allow different sampling generators to be combined with
modules that evaluate and report test results in the most appropriate way for the situation at hand.
This would be best implemented as a \emph{framework} 
based on common interfaces for generating, evaluating and reporting.
This would support:
\begin{enumerate}
\item research into automated test case generation
by allowing direct comparisons of test case selection criteria
and the development of new sampling methodologies
\item the use of sampling strategies for practical software verification 
on projects of different complexity and stages of software development.
\end{enumerate}

This chapter provides the requirements that a generic \pbt system framework should satisfy,
guided by the critiques of the other property based testing tools from chapter \ref{pbtsystems},
but also by the analysis of test context supporting hypotheses (section \ref{pbt}) and 
sampling methodologies from section \ref{sec:sampling_theory}.
The requirements cover:
\begin{enumerate}
\item the definition of the properties to be tested
\item the organization of test cases
\item the automatic generation of test cases
\item scheduling the evaluation of properties over test cases
\item reporting the results of the test
\end{enumerate}
\noindent
For the purposes of establishing a concrete set of recommendations
(and comparison with the \GC framework design and implementation, chapter \ref{chap:source})
it will be assumed that both the modules to be tested and
the \pbt framework will be written in Haskell,
but the requirements will apply to other similar functional programming languages.

\section {Properties} \label{reqprop}

Properties will be defined by the tester and passed to the test framework,
Properties will be defined as follows:

\begin{description}
\item[Boolean valued]
\item[univariate] 
\item[defined over their domain]  (section \ref{formal_pbt});
the function may be undefined outside of the specified property domain.
\item[(optional) precondition] boolean valued implementation of the characteristic function for the domain with respect to the property's type.
\end{description}


\subsection{Property Function and Specifications}
A property is a boolean valued function
defined over the domain of the property,
and each property will be evaluated individually.
Since the overall goal of the testing is 
likely to be the validation of the modules against a 
specification consisting of a number of properties,
the test framework should allow test results
over multiple properties to be combined into a single verdict.

\subsection{Univariate Properties}
The general practice in Haskell and other functional languages
is to \emph{curry} functions of multiple arguments (\cite{Haskell2010}),
Functions are ``first class citizens'',
so a property with multiple arguments
can be tested by fixing the first argument and 
then deriving the verdict on the segmented property over the remaining arguments.
\QC, for example, supports the testing of curried multivariate predicates in this way,
which is appropriate because only random test case generation is supported,
and each argument for a test case is generated independently.

This approach of fixing some input arguments and then generating test sets for the others
does not allow systematic sampling methods to be used effectively,
because the domain is not known in its entirety during test case generation.
It is also a problem with random sampling for enumerative generators,
which rely on having the complete definition of the test domain to
establish the probability distributions for selection.

Properties should instead be defined as \emph{univariate}
by uncurrying as required and sampling over the entire test domain.
Uncurrying inputs needed by the property
may prove a minor inconvenience to the test developer,
but does not otherwise impact the implementation being tested
(note that there is no suggestion that the modules being tested are uncurried, just the property).

\subsection{Property Domains and Preconditions}
The univariate parameter to the property will be specified by a type,
but the property will not necessarily be defined over all of the values of that type.
For example, a property might apply only to ordered lists,
but the condition of ordered cannot be represented in the type system.
The property must be defined over the domain required by the specification, 
but may be otherwise undefined.

A \emph{precondition} is a Boolean guard over the input type of the property function,
i.e. the \emph{characteristic} function of the property's domain with respect to the input type.
\QC and other packages incorporated these preconditions into the property definition
using a conditional combinator,
replacing test cases that did not satisfy the precondition with alternative values.
The representation of properties should therefore allow for the definition of a precondition
that can determine the validity of an input argument with respect to the specification.

\subsection{Existential Variables - Not a Requirement}

\SC and \GAST supported properties that incorporated \emph{existential} variables
with the goal of ensuring that at least (or exactly) one value exists that satisfies the property.
This is an appealing feature,
but solving for the existence of a value is substantially different than evaluating a property,
and as such has \emph{not} been included as a requirement.

Existential properties can instead be emulated by testing the negated property
e.g. $\exists x . p (x) \iff \neg (\forall x . \neg p (x))$.
The investigation of sampling strategies to support the correctness of existential properties
would be an interesting use a \pbt framework.

\section{Test Cases, Suites and Rank} \label{reqtestsuite}

The core functionality of a \PBT is to evaluate the property over 
a collection of test cases, here called the test suite.
The organization of these test cases will be used 
\begin{itemize}
	\item organize test cases by complexity 
	\item maintains the identity of the strata in a stratified sampling strategy
	\item support parallel evaluation on distributed systems
	\item support coverage analysis in reporting of results
	\item allows verdicts to be evaluated over parts of the test cases
\end{itemize}

\subsection{Test Cases}

Each test case will consist of:

\begin{description}
	\item[datum] the value over which the property will be evaluated,
	\item[meta-data] additional information about that value, including the rank
\end{description} 
\noindent
A test framework should allow different test case data structures,
storing different types of meta-data,
by defining a standard interface to access the datum.
This would allow customized test system components,
to share meta-data without requiring other components to be modified.

\subsection{Rank as a Complexity Measure }\label{sub:reqrank}

As discussed throughout this thesis,
term complexity plays an important role
in the selection, generation and evaluation priority of test cases,
particularly for recursive data types.
Using a complexity measure to partition the type's values into finite subsets
supports the automatic generation of test cases
but also ties the test results back to the supporting uniformity and regularity hypotheses
required for a formal argument of validity.
The complexity measure, which will be called the \emph{rank} of the term,
should be a mandatory part of the meta-data for a test case,
and will play an important role in generation, evaluation and reporting.

Different complexity measures have been implemented in different \pbt systems
(as discussed in section \ref{complexgen}),
and the framework should allow any complexity measure as the term ranking
as long as it is:
\begin{enumerate}
	\item represented by a Natural number (called the \emph{rank} of the term)
	\item strictly positive for any defined term; $\bot$ may have rank 0 if the test program were to support such a test value (e.g. \LSC)	
	\item $1$ for base (scalar) types such as Ints or Chars 
	\item a partial ordering of the terms
	\item strictly monotonic with respect to term inclusion,
	i.e. if the term |x| is constructed including the term |y|, the complexity of |x| should be strictly greater than that of |y|
	\item the range of the complexity measure should be positive natural numbers,
	which will be called the \emph{rank} of the terms
	\item a partition of the terms, with a finite number of terms for any given rank
\end{enumerate}
\noindent
Note that the use of naturals as the representation of the complexity aligns
the test results to inductive proofs used in proving program correctness.

There should be no requirement that the same complexity measure be used
within the same test, or even the same test suite,
to support research into complexity measures.

\subsection{Test Suites}

The test suite is an abstract container that is used to
organize to the test cases for 
scheduling their evaluation and / or reporting.
Test suites should have the following attributes:

\begin{description}
\item[Partitioned] allow an arbitrary partitioning of the test cases
\item[Labels] permit arbitrary labels for the parts
\item[Ordered] maintain the order of test cases within each part
\item[Immutable] neither test suite nor individual test cases should be modified by test evaluation
(this is natural for any system written in Haskell in any case)
\item[Persistent] storable and recoverable so tests can be repeated
\item[Duplicates] duplicate test values should be allowed as separate test cases
(thus the name ``suite'' instead of the theoretical test set from chapter \ref{chp:propertytesting})
\item[Merging] it should be possible to merge test suites (of like structure)
with the partitions and labels respected by the merge.
\item[Source Independent] the test suite should allow test cases from multiple sources,
whether automated generators, loaded from files or otherwise provisioned.
\end{description}
\noindent
These attributes are independent of the property being tested and the evaluation strategy.

The labeling may be based on 
any meta-data associated with the stratification strategy,
but should generally include the rank of the test cases within the part.

A test suite can be identified as \emph{valid} for a property if 
all of its test cases are known to be valid for a property test,
i.e. are within the property's domain.
This attribute of a property / test suite pair 
should be accepted by the framework and made available to the evaluation engine
to determine if the property's precondition should be applied before evaluating test cases.


\subsection{Multiple Properties and Shared Test Suites}

Properties with a common input argument type
should be able to use the same test cases to avoid the overhead of regenerating cases.
This is a weakness of \QC et al:
test cases are generated during test execution,
a new test suite is generated with every test pass
(the test cases can only be shared by testing the conjunction of the properties).
Sharing test suites requires they are accessible from outside the context of a single property test,
possibly by scheduling multiple properties for evaluation within the same test.

Repeatability is also an important requirement for testing,
so it must be possible to store and retrieve the suite to allow the test to be repeated.


\section{Automated Test Generation} \label{reqtestgen}

Automated test case generation for property based testing consists of :

\begin{enumerate}
	\item defining some sampling strategy defining the count, complexity and sampling methods to be used
	\item the application of sampling selection criterion, possibly including preconditions, to decide which values to evaluate and in what order
	\item building the concrete instances of test datum of the appropriate type
\end{enumerate}

A property based testing framework should provide a
standardized but powerful set of tools to generate test cases
for all representable types and standard sampling methods (section \ref{sec:sampling_theory}).
This should include both a library of generators of standard types and sampling methods,
and the ability to building new or customizing generators,
including the use of non-standard sampling methods.
As shown in chapter \ref{chp:enumgen}, 
automated test case generation in Haskell is amenable to standardizing
because generators for algebraic data types which form the majority of the terms
may be created using datatype generic construction techniques.

\subsection{Generators}

A test case \emph{generator} is a function that produces an ordered collection of test cases of a fixed type.
These were examined at length in chapter \ref{chp:enumgen}
where scalar, parameterized and complexity ranked generators were defined;
the requirements are repeated in summary form here for convenience.

Generators are functions returning lists of terms of a given type.
\begin{description}
	\item[scalar] generators return values of a scalar type and do not require rank as an input argument
	\item[complexity ranked] generators produce terms of a specified rank
	\item[parameterized] generators that accept arguments other than rank
\end{description}

A generator will apply a sampling method to select and order the values to be returned.
For recursive or infinitely populated types (e.g. unlimited Integers),
the sampling method will be applied to the values of a given complexity / rank.
The framework should support the use of standard sampling techniques:
\begin{enumerate}
	\item exhaustive
	\item random
	\item uniform
	\item extreme / boundary
\end{enumerate}
In addition, it should be possible to use alternative sampling methods in generators
and incorporate those new methods into the test strategies in the system.

The framework must accept a wide range of behaviors from generators, which may:
\begin{enumerate}
	\item produce a finite or unbounded sequence of values,
	\item guarantee unique values or permit duplicates,
	\item have a range that does or does not include all of the type's values,
	\item order values according to their priority for testing,
\end{enumerate}


\subsection{ Generator Substitution and Composition } \label{sub:reqsubcomp}

\gordon{Is this really a requirement, or is it just part of the GenCheck implementation?}

Mutual recursion, substitution and composition are all used
to define data types in Haskell programs.
Any of these could be complicated relationships,
and it should be possible to choose different sampling strategies
for components, elements and mutual recursive structures.

One of the most significant shortcomings of the single sampling strategy packages
is their inability to generate efficient test suites of data structures populated with base elements.
The problem is that exhaustive testing of algebraic data types 
is a good strategy for the \emph{shapes} of the data types,
but generally not efficient for the base type elements such as integers in the structures.
Random generators do well producing the elements,
but then don't provide the same confidence and efficiency when testing
the small / simple structure shapes that exhaustive testing would provide.

The testing framework should allow 
structures to be generated just as shapes first,
and then substitute independently generated sets of elements to populate them.
It should also allow the elements to be generated as 
part of the structure, i.e. allow the enumeration or traversal to \emph{include element values}.
Structures and their elements are fundamentally different
so the test generators should allow different sampling strategies 
to select structures and their of elements.
Generators could then populate the structures substituting 
one or more sets of elements into each structure.

Generators should be able to use at least the following five \emph{substitution} strategies:

\begin{enumerate}
\item one distinct substitution per structure
\item $n$ distinct substitutions per structure
\item linearly partition the set of elements by the structure size
and populate as many copies of the structure as there are partitions (finite generators only)
\item all  possible permutations of a set of elements per structure
\item all combinations of a set of elements per structure
\end{enumerate}

Composition is similar to substitution but replaces the elements of the data structure
with other data structures; the newly composed structure can then 
be populated with elements through substitution.
In substitution, the rank of the substitution set is ignored,
and all of the values are assumed to be of rank 1.
In composition the substitution set values are structures with a variable rank,
and the total rank (or complexity) of the composed structure
is the rank of the initial structure plus the sum of the ranks of the composing structures.

\gordon{picture of substitution and composition, composition showing the sum of ranks}


\subsection{Sampling Strategies}

Evaluation of the existing property based testing software showed that 
the most significant distinguishing feature was the test selection criterion.
Each of these sampling methods has advantages and disadvantages,
proponents and detractors, and a role to play in proving the test hypothesis.
Other sampling methods are often presented, usually as heuristics,
and there is no definitive evaluation process to determine which is best.
It is reasonable to conclude that test selection criterion are still
an area of research and that any test framework should support
all of them and other methodologies not discussed here.

It is also reasonable to demand that a test framework support
multiple sampling methods in a single test data set.
Most practitioners would agree that testing a module
using both \QC and \SC would provide greater confidence
than using either alone; being able to use the same specification
and implementation to do both tests is convenient.
It would be even more convenient if the default for testing a property
was to incorporate multiple sampling methods without
having to maintain multiple \pbt systems and separate test results.

There is a ``strategy'' in the construction of a test suite
that can be defined independently of the specific type of the test case.
The strategy will specify one or more parts to the test,
with each part defined by a sampling method, 
a number of test cases (perhaps as a percentage of the total test case count)
and a range of term complexity for ranked types.
A \pbt framework should allow the definition of such a strategy independently of the test type,
and be able to either retrieve or construct the generators required
where the sampling methods are known (e.g the standard sampling methods above).


\section{Evaluation and Scheduling}

A test evaluates some or all of the test cases in a test suite,
using an evaluation function.
There are different kinds of tests depending on 
the characteristics of the evaluation function,
and the termination conditions for the test.

A test for a property consists of evaluating a property
over the data contained in a test suite.
Assuming that the test cases are independent 
the test program must decide how to schedule test cases for evaluation
and collect the results.


\begin{df}[Test]
A test is a function from a test suite to a result based on an evaluation function:

$$\test{\eval{\property}} : \testsuite{\alpha} \ra \resultset{\alpha}$$
\end{df}


\begin{df}[Conditional Test]
A conditional test is based on a conditional evaluation function $\eval{\property}$ 
and may include test cases assigned the verdict $\invalid$ where
the test data is not in the property's domain.
\end{df}
\begin{df}[Unconditional Test]
An unconditional test is based on an unconditional evaluation function $\evaluncon{\property}$, 
so all test data is assumed to be in the property's domain
and will not be identified as $\invalid$.
\end{df}

\begin{df}[Complete Test]
A test is complete if all of the test cases are evaluated,
so no test cases will have the verdict $\noteval$.
\end{df}
\begin{df}[Partial Test]
A partial test may terminate without evaluating all of the test cases,
so some test cases may have the verdict $\noteval$.
\end{df}

\begin{df}[Time Limited]
A test is time limited if an exception is generated when
the evaluation of a property (or a precondition in the form of a domain's characteristic function)
fails to evaluated in a set period of time,
resulting in a verdict of $\nonterm$ for that case.
\end{df}

A test function can be characterized by these three concepts
(conditional / unconditional, complete / partial, time limited or not) independently.

\subsection{Evaluation function}

An evaluation function for the property P
applies the property function to the datum of a test case.
The following factors should be considered:

\begin{description}
\item[Conditional] evaluation functions will accept as input the characteristic function of a property's domain,
identify any test cases that are not valid and return $\invalid$ without evaluating the property at that value.
\item[Unconditional] evaluation functions does not perform such a check,
and are used if the test cases are guaranteed to be in the domain,
or if it is more efficient to evaluate the invalid test cases
than to prevent them from being tested.
\item[Time-limited] evaluation functions will interrupt the evaluation of a test case
if it exceeds a time limite and produce a $\nonterm$ verdict.
This could also be caused by the characteristic function not terminating,
if domain membership is being tested,
but these conditions will not be distinguished.
A $\nonterm$ result could be interpreted as a $\fail$ verdict,
but this is not an accurate reporting of the results.
\end{description}

Note that if the characteristic function for the domain is available,
an evaluation function maybe constructed by composing
the characteristic function of the domain (precondition) and 
an unconditional evaluation function:


\subsubsection{Scheduling}
Scheduling refers to providing the order in which test cases in a test suite are evaluated.
An implementation of a test function will
determine how the evaluation function will be applied
to the test cases in the test suite.
If it is a partial test, the scheduling will include
how termination conditions for the test are implemented.

The simplest form of scheduling is 
a |map| of the evaluation function
over the test suite container.
This allows the compiler to assign any order
to the evaluations and so may be more efficient.
This is a pure computation (i.e. not monadic) 
so has the property of referential transparency.
In an Haskell implementation,
this would require the test suite container to be an instance of the |Functor| class.

\QC, \SC and the related packages reviewed in chapter \ref{pbtsystems}
scheduled test cases to be generated and evaluated sequentially,
using a monadic test function (or a similar approach).
This kind of scheduling can be accomplished using 
a |fold| over the test suite container.
In an Haskell implementation,
this would require the test suite container to be an instance of the |Foldable| class;
this class will take advantage of the interior monoidal construction of the test suite
described above in section \ref{reqtestsuite}.

The evaluation of a test case is the process of computing 
whether the property holds at that value.
and possibly their distribution amongst different threads, nodes or systems as required;
on distributed systems, it would also include collecting the results for reporting.
These are generally simple steps,
but some care to make sure
Haskell's lazy evaluation environment does not confuse 
when and how that this happens.

\subsection{Execution Mode}

The programming interface should function without assumed access to the IO environment,
except for components that exist solely to perform IO
(or are use to test implementations that perform IO).
A test of a pure implementation should be a pure computation itself,
with only a thin layer of monadic control provided for displaying the results.
When the property being tested incorporates monadic functions,
the property should be written to resolve the monad.
The IO monad prevents this, so then the property must be tested in a monadic way.

\gordon{think about this some more, testing monadic functions, look up papers}

Reporting of course must be an IO function,
as is retrieving test cases from a file,
but otherwise the test components should not use an IO monad

\subsubsection{Parallelism}
Testing is one of the few intrinsically parallel tasks software engineers face,
as test cases are (at least meant to be) independent.
It is important to have the opportunity to take advantage of the increase resources this offers.

\subsection{Termination Conditions}

\QC, \SC, etc. terminate testing when a fixed number (defaulting to 1) of errors are found.
This is a reasonable approach for quick checks during coding and debugging,
and results in a small number of failure cases being reported so is easy to interpret.
It does require that the test cases are evaluated sequentially,
or perhaps results from that approach to testing.

More formal software development environments
will often complete the test and report on all test cases,
as the process for correcting, building and testing the system is much longer,
and so gathering as much information about failed test cases as possible is desirable.
Of course, it is only useful if the test reports use sensible approaches to 
consolidating or summarizing the results when there are a large numbers of errors.

Testing programs should allow time limits to be set on 
the evaluation of an individual test case,
and on the overall amount of time devoted to a test suite.
This both identifies non-terminating test cases
and allows very large test suites to be defined
and evaluated for the timeframe desired
instead of trying to guess how many test cases should be run.
This approach will work well with Haskell's lazy evaluation.


\section{Results and Reporting}
The results of a test - the evaluation of a property over a test suite - consist of the overall verdict,
but also the evaluation of each test case 
and optionally information about the performance of the system during that evaluation.
Like the test case,
a result should be a class of data structure,
with a method to retreive the verdict,
but optional meta-data that can be captured by a specialized evaluation engine
and made available to a reporting module.

Although the properties are boolean valued functions,
the verdict of a test must be expanded beyond a simple success / fail
to capture the possible outcomes associated with a test case:

\begin{df}[Verdicts]
	The \emph{verdict} of a test is a set of labels describing the combined results of one or more test evaluations:
	
	$\verdictset = \{ \fail, \nonterm, \success, \invalid, \noteval \}$ where
	
	\begin{description}
		\item[$\fail$] the property does not hold over the valid test case
		\item[$\nonterm$] the property did not evaluate in the allowed time
		\item[$\success$] the property holds over the valid test case
		\item[$\invalid$] the test case was not in the domain of P, so was not evaluated
		\item[$\noteval$] the test case was not evaluated by the test (may or may not be valid)
	\end{description}
\end{df}

Not all verdicts will be possible under all models of evaluation.
The $\nonterm$ verdict in particular will only occur if the evaluation engine 
sets some kind of time or resource limit on execution.
The other states are all used by at least one of the packages from chapter \ref{pbtsystems}.

These verdicts are actually in a precedence order that can be used when combining 
the verdicts of two result sets,
e.g. a fail verdict overrides non-termination, non-termination overrides success, etc.
\begin{df}[Verdict Monoid]
	identity : \noteval \\
	concat : v1 \^ v2 = greater of the two\\
\end{df}

The result container consists of the verdict and evaluations of each test case.
Making the result container a labeled partitioned container like the test suite
provides good support for reporting coverage analysis,
and provides for low overhead evaluation strategies (such as map),
but also allows the result set to be monoidal in both the container and the verdict.

\begin{df}[Result Class]
	defines a labelled partioned container as defined for the test suite,
	but the monoid is extended to include the calculation of a verdict.
	
	verdict is available for any part or single test result.
\end{df}
Note that while there may be some confusion in a Haskell implementation
caused by having verdicts at both the test case and test suite levels,
the monoidal nature of the verdict computation makes the meaning consistent.

Reporting modules will present the contents of a results class,
optionally using the partitioning and labeling of those parts
to provide more detailed analyses.
They may be specialized to use extra information about the evaluations.

%
%\section{Summary}
%
%It is impossible to generalize the needs of any significant project size,
%and it is equally unnecessary to trap the tester in any given paradigm.
%Data generation should not be restricted to any particular theory, 
%container type, monad or sampling strategy.
%It should allow a significant level of choice and customization by the tester.
%This is can best be managed by developing an architecture
%and framework for assembling test programs from components providing
%generators, test suites, evaluation and reporting functions through
%minimal programming interfaces. The testing software should provide support for :
%
%\begin{enumerate}
%\item quickly building default generators from the property definitions
%\item quickly build default test suites using mixed heuristic strategies
%\item allow test suites to be combined and separated, stored and recovered 
%\item allow test suite strategies consisting of sampling strategies over known generators
%\item allow generators to be customized using hybrid sampling strategies 
%for element substitution, structure composition or mutual recursion
%\item allow fully customgenerators to be included at any point,
%\item support inclusion of manually coded test cases
%\item integration with different execution models by decoupling the
%test scheduling from execution and generation, except where absolutely necessary
%\item integration of different reporting modules that allow varying degrees of 
%coverage analysis and interactivity
%\end{enumerate}
%
